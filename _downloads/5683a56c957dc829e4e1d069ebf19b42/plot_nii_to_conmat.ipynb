{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Compute Conmat from nii files properties from a given connectivity matrix\n\nThe conmat_to_graph pipeline performs graph analysis .\n\nThe **input** data should be a symetrical connecivity matrix in **npy** format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: David Meunier <david_meunier_79@hotmail.fr>\n\n# License: BSD (3-clause)\nimport os.path as op\n\nimport nipype.pipeline.engine as pe\n\nfrom nipype.interfaces.utility import IdentityInterface\nimport nipype.interfaces.io as nio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if data are available\nneeds to import neuropycon_data\n'pip install neuropycon_data' should do the job...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import neuropycon_data as nd\nexcept ImportError:\n    print(\"Warning, neuropycon_data not found\")\n    exit()\n\ndata_path = op.join(nd.__path__[0], \"data\", \"data_nii\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create our workflow and specify the `base_dir` which tells\nnipype the directory in which to store the outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# workflow directory within the `base_dir`\nconmat_analysis_name = 'conmat'\n\nROI_mask_file = op.join(data_path,\"ROI_HCP\",\"indexed_mask-ROI_HCP.nii\")\nROI_coords_file = op.join(data_path,\"ROI_HCP\",\"ROI_coords-ROI_HCP.txt\")\nROI_MNI_coords_file =op.join(data_path,\"ROI_HCP\",\"ROI_MNI_coords-ROI_HCP.txt\")\nROI_labels_file = op.join(data_path,\"ROI_HCP\",\"ROI_labels-ROI_HCP.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create a node to pass input filenames to DataGrabber from nipype\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject_ids = ['01']\nfunc_sessions = ['rest']\ninfosource = pe.Node(interface=IdentityInterface(\n    fields=['subject_id','session']),\n    name=\"infosource\")\n\ninfosource.iterables = [('subject_id', subject_ids),\n    ('session', func_sessions)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and a node to grab data. The template_args in this node iterate upon\nthe values in the infosource node\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# template_path = '*%s/conmat_0_coh.npy'\n# template_args = [['freq_band_name']\n# datasource = create_datagrabber(data_path, template_path, template_args)\n\ndatasource = pe.Node(\n    interface=nio.DataGrabber(infields=['subject_id','session'],\n                              outfields=['img_file']),\n    name='datasource')\n\ndatasource = pe.Node(interface=nio.DataGrabber(\n    infields=['subject_id','session'],\n    outfields= ['img_file','gm_anat_file','wm_anat_file','csf_anat_file']),\n    name = 'datasource')\n\ndatasource.inputs.base_directory = data_path\ndatasource.inputs.template = '%ssub-%s%s%s%s'\ndatasource.inputs.template_args = dict(\nimg_file=[[\"wr\",'subject_id',\"_task-\",'session',\"_bold.nii\"]],\ngm_anat_file=[[\"wc1\",'subject_id',\"\",'',\"_T1w.nii\"]],\nwm_anat_file=[[\"wc2\",'subject_id',\"\",'',\"_T1w.nii\"]],\ncsf_anat_file=[[\"wc3\",'subject_id',\"\",'',\"_T1w.nii\"]],\nrp_file=[[\"rp_\",'subject_id',\"_task-\",'session',\"_bold.txt\"]],\n       )\n\ndatasource.inputs.sort_filelist = True\n\n#0/0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This parameter corrdesponds to the percentage of highest connections retains\n# for the analyses. con_den = 1.0 means a fully connected graphs (all edges\n# are present)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#import json  # noqa\n#import pprint  # noqa\n\n#data_graph = json.load(open(\"params_graph.json\"))\n#pprint.pprint({'graph parameters': data_graph})\n\n## density of the threshold\n#con_den = data_graph['con_den']\n\n## The optimisation sequence\n#radatools_optim = data_graph['radatools_optim']\n\nfrom graphpype.pipelines.nii_to_conmat import create_pipeline_nii_to_conmat # noqa\nfrom graphpype.pipelines.nii_to_conmat import create_pipeline_nii_to_conmat_seg_template # noqa\nfrom graphpype.pipelines.nii_to_conmat import create_pipeline_nii_to_subj_ROI #noqa\n\nmain_workflow = pe.Workflow(name= conmat_analysis_name)\nmain_workflow.base_dir = data_path\n\nconf_interval_prob = 0.05\n\n###### time series and correlations\ncor_wf = create_pipeline_nii_to_conmat(main_path=data_path, conf_interval_prob=conf_interval_prob, resample=True, background_val=0.0)\n\n#cor_wf = create_pipeline_nii_to_conmat_seg_template(main_path =\n#data_path, conf_interval_prob = conf_interval_prob)\n\n#cor_wf = create_pipeline_nii_to_subj_ROI(main_path =\n#data_path, resample = True)\n\n\nmain_workflow.connect(datasource, 'img_file', cor_wf, 'inputnode.nii_4D_file')\nmain_workflow.connect(datasource, 'gm_anat_file', cor_wf, 'inputnode.gm_anat_file')\nmain_workflow.connect(datasource, 'wm_anat_file', cor_wf, 'inputnode.wm_anat_file')\nmain_workflow.connect(datasource, 'csf_anat_file', cor_wf, 'inputnode.csf_anat_file')\nmain_workflow.connect(datasource, 'rp_file', cor_wf,'inputnode.rp_file')\n\ncor_wf.inputs.inputnode.ROI_mask_file = ROI_mask_file\ncor_wf.inputs.inputnode.ROI_coords_file = ROI_coords_file\ncor_wf.inputs.inputnode.ROI_MNI_coords_file = ROI_MNI_coords_file\ncor_wf.inputs.inputnode.ROI_labels_file = ROI_labels_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then connect the nodes two at a time. We connect the output\nof the infosource node to the datasource node.\nSo, these two nodes taken together can grab data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.connect(infosource, 'subject_id', datasource, 'subject_id')\nmain_workflow.connect(infosource, 'session', datasource, 'session')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# To do so, we first write the workflow graph (optional)\nmain_workflow.write_graph(graph2use='colored')  # colored\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# and visualize it. Take a moment to pause and notice how the connections\n# here correspond to how we connected the nodes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#from scipy.misc import imread  # noqa\n#import matplotlib.pyplot as plt  # noqa\n#img = plt.imread(op.join(data_path, graph_analysis_name, 'graph.png'))\n#plt.figure(figsize=(8, 8))\n#plt.imshow(img)\n#plt.axis('off')\n#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we are now ready to execute our workflow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\n\nmain_workflow.run()\n\n# Run workflow locally on 2 CPUs\n#main_workflow.run(plugin='MultiProc', plugin_args={'n_procs': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# plotting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#from graphpype.utils_visbrain import visu_graph_modules # noqa\n\n#labels_file = op.join(data_path, \"correct_channel_names.txt\")\n#coords_file = op.join(data_path, \"MNI_coords.txt\")\n\n#from visbrain.objects import SceneObj, BrainObj # noqa\n\n#sc = SceneObj(size=(1000, 1000), bgcolor=(.1, .1, .1))\n\n#for nf, freq_band_name in enumerate(freq_band_names):\n\n    #res_path = op.join(\n        #data_path, graph_analysis_name,\n        #\"graph_den_pipe_den_\"+str(con_den).replace(\".\", \"_\"),\n        #\"_freq_band_name_\"+freq_band_name)\n\n    #lol_file = op.join(res_path, \"community_rada\", \"Z_List.lol\")\n    #net_file = op.join(res_path, \"prep_rada\", \"Z_List.net\")\n\n    #b_obj = BrainObj(\"white\", translucent=False)\n    #sc.add_to_subplot(\n        #b_obj, row=nf, use_this_cam=True, rotate='left',\n        #title=(\"Module for {} band\".format(freq_band_name)),\n        #title_size=14, title_bold=True, title_color='white')\n\n    #c_obj,s_obj = visu_graph_modules(lol_file=lol_file, net_file=net_file,\n                               #coords_file=coords_file,\n                               #labels_file=labels_file, inter_modules=False,\n                               #z_offset=+50)\n    #sc.add_to_subplot(c_obj, row=nf)\n    #sc.add_to_subplot(s_obj, row=nf)\n\n\n#sc.preview()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}